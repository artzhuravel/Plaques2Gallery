{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a667394",
   "metadata": {},
   "source": [
    "# Plaques2Gallery Pipeline\n",
    "\n",
    "This notebook orchestrates the full pipeline for matching museum plaque photographs to artwork images.\n",
    "\n",
    "**Pipeline Overview:**\n",
    "1. **OCR Text Extraction** — Extract text from plaque images using Tesseract\n",
    "2. **Text Cleaning** — Use Gemini AI to parse \"Title by Artist\" from noisy OCR output\n",
    "3. **Web Search** — Find candidate URLs using Google Custom Search API\n",
    "4. **Image Extraction** — Download artwork images from the found URLs\n",
    "\n",
    "**Prerequisites:**\n",
    "- Ensure `.env` file contains your API keys (see `.env.example`)\n",
    "- Place plaque images in the `museum_plaques/` directory\n",
    "- Run cells sequentially within each section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i38o22j8rm",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Load all required libraries and configure the environment. The `load_dotenv()` call reads API keys from your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecee4ff",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\nimport shutil\nfrom datetime import datetime\n\n# Add parent directory to path so we can import the plaques2gallery package\nsys.path.insert(0, str(Path().resolve().parent))\n\nimport os\nimport json\nimport cv2\nimport numpy as np\nimport pytesseract\nimport re\nfrom dotenv import load_dotenv\nfrom plaques2gallery.ocr_text_extraction import invert, custom_binarize\nfrom plaques2gallery.clean_museum_plaques_text import clean_extracted_text\nfrom plaques2gallery.web_search import google_search_top3\nfrom plaques2gallery.web_scraping import process_a_painting\nimport google.generativeai as genai\nfrom playwright.async_api import async_playwright\nimport asyncio\nfrom docx import Document\nfrom docx.shared import Inches\nfrom make_it_pretty import save_results_to_docx\n\nload_dotenv()\n\n# Create required directories\nos.makedirs(\"intermediate_results\", exist_ok=True)\nos.makedirs(\"extracted_images\", exist_ok=True)\n\n\ndef save_json_with_backup(filepath: str, data: dict) -> None:\n    \"\"\"Save JSON data with automatic backup of existing file.\"\"\"\n    if os.path.exists(filepath):\n        backup_dir = \"intermediate_results/backups\"\n        os.makedirs(backup_dir, exist_ok=True)\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = os.path.basename(filepath)\n        backup_path = os.path.join(backup_dir, f\"{filename}.{timestamp}.bak\")\n        shutil.copy2(filepath, backup_path)\n    \n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, ensure_ascii=False, indent=2)"
  },
  {
   "cell_type": "markdown",
   "id": "4a728cf8",
   "metadata": {},
   "source": [
    "## 1. OCR Text Extraction\n",
    "\n",
    "Extract text from museum plaque images using Tesseract OCR.\n",
    "\n",
    "**Process:**\n",
    "1. Load previously processed results (to avoid re-processing)\n",
    "2. For each new image: convert to grayscale, invert if needed, binarize\n",
    "3. Run OCR with multi-language support (English, German, Italian, Korean, Chinese, Japanese)\n",
    "4. If standard thresholding fails, try adaptive thresholding to maximize OCR confidence\n",
    "\n",
    "**Output:** `intermediate_results/ocr_extracted_text.json` — mapping of image filenames to raw OCR text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e240cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'museum_plaques'\n",
    "\n",
    "try:\n",
    "    with open(\"intermediate_results/ocr_extracted_text.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        ocr_extracted_text = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    ocr_extracted_text = {}\n",
    "\n",
    "newly_extracted_text = {}\n",
    "for filename in os.listdir(folder_name):\n",
    "    if filename in ocr_extracted_text:\n",
    "        continue\n",
    "\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "        img_path = os.path.join(folder_name, filename)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        inverted = invert(gray)\n",
    "        t, binary = cv2.threshold(inverted, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "        text = pytesseract.image_to_string(binary, lang='eng+deu+ita+kor+chi_sim+jpn')\n",
    "        if not text:  # Try custom thresholding in the case of a failure\n",
    "            t, binary = custom_binarize(inverted)\n",
    "            text = pytesseract.image_to_string(binary, lang='eng+deu+ita+kor+chi_sim+jpn')\n",
    "        \n",
    "        if not text: text = \"Failed to extract text.\"\n",
    "        \n",
    "        newly_extracted_text[filename] = text.strip()\n",
    "\n",
    "ocr_extracted_text.update(newly_extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826dc450",
   "metadata": {},
   "outputs": [],
   "source": "save_json_with_backup(\"intermediate_results/ocr_extracted_text.json\", ocr_extracted_text)"
  },
  {
   "cell_type": "markdown",
   "id": "e7bab976",
   "metadata": {},
   "source": [
    "## 2. Text Cleaning with Gemini AI\n",
    "\n",
    "Raw OCR output is often noisy and inconsistent. This step uses Google's Gemini model to extract a clean, standardized format: **\"Painting Title by Artist\"**.\n",
    "\n",
    "**Process:**\n",
    "1. Load the Gemini API key from environment variables\n",
    "2. For each OCR result, prompt Gemini to extract only the title and artist\n",
    "3. The model handles OCR artifacts, formatting issues, and multilingual text\n",
    "\n",
    "**Output:** `intermediate_results/cleaned_names.json` — mapping of image filenames to cleaned \"Title by Artist\" strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccc5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate_results/ocr_extracted_text.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ocr_extracted_text = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a750198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable is not set. Please add it to your .env file.\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel(\"models/gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ef8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"intermediate_results/cleaned_names.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        cleaned_names = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    cleaned_names = {}\n",
    "\n",
    "newly_cleaned_names = {}\n",
    "for img_name, ocr_text in newly_extracted_text.items():\n",
    "    if ocr_text == \"Failed to extract text.\":\n",
    "        newly_cleaned_names[img_name] = \"Failed to extract text.\"\n",
    "        continue\n",
    "    newly_cleaned_names[img_name] = clean_extracted_text(ocr_text, model)\n",
    "\n",
    "cleaned_names.update(newly_cleaned_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3fa53",
   "metadata": {},
   "outputs": [],
   "source": "save_json_with_backup(\"intermediate_results/cleaned_names.json\", cleaned_names)"
  },
  {
   "cell_type": "markdown",
   "id": "a243247d",
   "metadata": {},
   "source": [
    "## 3. Web Search for Artwork Images\n",
    "\n",
    "Use Google Custom Search API to find candidate URLs for each artwork.\n",
    "\n",
    "**Why batching?** The free tier of Google Custom Search API allows only 100 queries/day. With hundreds of plaques, we split them into batches of 70 and process one batch per day.\n",
    "\n",
    "**Process:**\n",
    "1. Split cleaned names into batches of 70\n",
    "2. Track which batch was last processed via `last_batch_index.txt`\n",
    "3. For each painting name, search Google and collect top 3 URLs\n",
    "4. Save results incrementally to avoid losing progress\n",
    "\n",
    "**Output:** `intermediate_results/search_results.json` — mapping of painting names to lists of candidate URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df30d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate_results/cleaned_names.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    imgs_to_names = json.load(f)\n",
    "\n",
    "new_imgs_to_names = imgs_to_names\n",
    "new_names_to_imgs = {painting_name: img for img, painting_name in new_imgs_to_names.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 70\n",
    "output_dir = \"intermediate_results/batches_for_searching\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Find the max existing batch index\n",
    "existing_files = os.listdir(output_dir)\n",
    "pattern = re.compile(r\"batch_(\\d+)\\.json\")\n",
    "existing_indices = [int(m.group(1)) for f in existing_files if (m := pattern.match(f))]\n",
    "new_batch_start_idx = max(existing_indices, default=0) + 1\n",
    "\n",
    "# Create and write new batches\n",
    "items = list(new_imgs_to_names.items())\n",
    "for i in range(0, len(items), batch_size):\n",
    "    batch = dict(items[i:i + batch_size])\n",
    "    batch_num = new_batch_start_idx + (i // batch_size)\n",
    "    batch_path = os.path.join(output_dir, f\"batch_{batch_num}.json\")\n",
    "    with open(batch_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(batch, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_SEARCH_API_KEY = os.getenv(\"GOOGLE_SEARCH_API_KEY\")\n",
    "CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")\n",
    "\n",
    "if not GOOGLE_SEARCH_API_KEY or not CSE_ID:\n",
    "    raise ValueError(\"GOOGLE_SEARCH_API_KEY and GOOGLE_CSE_ID environment variables must be set. Please add them to your .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621572b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current batch to process. One should be very careful with this step.\n",
    "idx_file = os.path.join(\"intermediate_results/batches_for_searching\", \"last_batch_index.txt\")\n",
    "try:\n",
    "    with open(idx_file, \"r\") as f:\n",
    "        cur_batch_idx = int(f.read().strip()) + 1\n",
    "except FileNotFoundError:\n",
    "    cur_batch_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_batch_file = os.path.join(\"intermediate_results/batches_for_searching\", f\"batch_{cur_batch_idx}.json\")\n",
    "if os.path.exists(cur_batch_file):\n",
    "    with open(cur_batch_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        cur_batch = json.load(f)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Batch file for index {cur_batch_idx} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d400598",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"intermediate_results/search_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        search_results = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    search_results = {}\n",
    "\n",
    "new_search_results = {}\n",
    "for _, painting_name in cur_batch.items():\n",
    "    links = google_search_top3(painting_name, GOOGLE_SEARCH_API_KEY, CSE_ID)\n",
    "    new_search_results[painting_name] = links\n",
    "\n",
    "search_results.update(new_search_results)\n",
    "\n",
    "with open(idx_file, \"w\") as f:\n",
    "    f.write(str(cur_batch_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078eb865",
   "metadata": {},
   "outputs": [],
   "source": "save_json_with_backup(\"intermediate_results/search_results.json\", search_results)"
  },
  {
   "cell_type": "markdown",
   "id": "18582a16",
   "metadata": {},
   "source": [
    "## 4. Image Extraction with Playwright\n",
    "\n",
    "Visit each candidate URL and download the artwork image using browser automation.\n",
    "\n",
    "**Process:**\n",
    "1. Load the current batch and its search results\n",
    "2. For each painting, try URLs in priority order: Wikipedia → Museum sites → Other\n",
    "3. Use Playwright to render the page (handles JavaScript-heavy sites)\n",
    "4. Select the largest visible image on the page (heuristic for finding the artwork)\n",
    "5. Handle cookie banners automatically\n",
    "6. Convert downloaded images to JPEG format\n",
    "\n",
    "**Output:**\n",
    "- `extracted_images/` — downloaded artwork images\n",
    "- `final_results.json` — complete results with image paths, source URLs, and museum info\n",
    "- `final_results.docx` — formatted document for easy viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07e5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we should be careful here with the cur_batch_idx. If needed, the index can be selected manually when running the code.\n",
    "cur_batch_file = os.path.join(\"intermediate_results/batches_for_searching\", f\"batch_{cur_batch_idx}.json\")\n",
    "if os.path.exists(cur_batch_file):\n",
    "    with open(cur_batch_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        cur_batch = json.load(f)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Batch file for index {cur_batch_idx} not found.\")\n",
    "\n",
    "search_results_file = 'intermediate_results/search_results.json'\n",
    "with open(search_results_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    full_search_results = json.load(f)\n",
    "\n",
    "# Get the artwork titles (values) from cur_batch\n",
    "cur_artwork_titles = set(cur_batch.values())\n",
    "\n",
    "# Filter only entries in full_search_results with matching keys\n",
    "cur_batch_search_results = {\n",
    "    title: urls for title, urls in full_search_results.items() if title in cur_artwork_titles\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17bdd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"final_results.json\", \"r\") as f:\n",
    "        final_results = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    final_results = {}\n",
    "\n",
    "cur_batch_results = {}\n",
    "\n",
    "async_groups_cur_batch = []\n",
    "cur_async_group = {}\n",
    "for painting_name, urls in cur_batch_search_results.items():\n",
    "    if len(cur_async_group) == 3:\n",
    "        async_groups_cur_batch.append(cur_async_group)\n",
    "        cur_async_group = {}\n",
    "    cur_async_group[painting_name] = urls\n",
    "\n",
    "if cur_async_group:\n",
    "    async_groups_cur_batch.append(cur_async_group)\n",
    "\n",
    "cur_batch_names_to_imgs = {painting_name: img for img, painting_name in cur_batch.items()}\n",
    "\n",
    "async with async_playwright() as p:\n",
    "    browser = await p.chromium.launch(headless=True)\n",
    "    for async_group in async_groups_cur_batch:\n",
    "        tasks = [\n",
    "            process_a_painting(painting_name, urls, browser, cur_batch_names_to_imgs, cur_batch_results)\n",
    "            for painting_name, urls in async_group.items()\n",
    "        ]\n",
    "        await asyncio.gather(*tasks)\n",
    "    await browser.close()\n",
    "\n",
    "final_results.update(cur_batch_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d72de",
   "metadata": {},
   "outputs": [],
   "source": "save_json_with_backup(\"final_results.json\", final_results)\n\ndoc_path = \"final_results.docx\"\nif os.path.exists(doc_path):\n    backup_dir = \"intermediate_results/backups\"\n    os.makedirs(backup_dir, exist_ok=True)\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    shutil.copy2(doc_path, os.path.join(backup_dir, f\"final_results.docx.{timestamp}.bak\"))\n\nsave_results_to_docx(doc_path, cur_batch_results)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}